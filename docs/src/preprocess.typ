== 数据分析与处理
#import "@preview/tablem:0.3.0": tablem
#import "@preview/mitex:0.2.6": mi, mitex
//== 文本向量化

=== TF-IDF文本处理



#tablem(
  columns: (1fr, 1.4fr, 1.8fr),
  align: (left, center, left),
)[
  | 步骤 | 操作 | 说明与效果 |
  | :--- | :--- | :--- |
  | 1. 文本提取 | `train_df["Phrase"].astype(str)` | 从数据框中提取评论文本，确保格式统一为字符串 |
  | 2. 标签提取 | `train_df["Sentiment"].astype(int)` | 提取情感标签（0-4），转换为整型用于监督学习 |
  | 3. 文本向量化 | `TfidfVectorizer(ngram_range=(1,2))` | 将每条文本转换为数值向量，同时考虑单词和双词组合 |
  | 4. 特征选择 | `max_features=50000` | 保留最重要的50,000个词汇特征，控制模型复杂度 |
  | 5. 停用词过滤 | `stop_words='english'` | 自动移除"the", "a", "is"等常见无信息量词汇 |
  | 6. 权重计算 | 自动计算TF-IDF值 | 基于词频和逆文档频率为每个词赋予重要性权重 |
  | 7. 分类器训练 | `LogisticRegression()` | 在TF-IDF特征上训练多项逻辑回归分类器 |
  | 8. 流水线封装 | `make_pipeline()` | 将向量化和分类步骤封装为统一对象，确保数据一致处理 |
]
---

=== TF-IDF处理流程总结


处理流程总结：
1. 输入：原始评论文本和情感标签
2. 转换：通过TF-IDF将文本转换为高维稀疏特征矩阵
3. 降维：限制特征数量至50,000维，平衡信息保留与计算效率
4. 建模：在转换后的特征上训练逻辑回归分类器
5. 输出：可重复使用的训练流水线，包含学到的词汇表和分类权重

---
=== BPE Tokenization


**BPE**（Byte Pair Encoding）通过迭代合并高频字符对构建子词词表，有效平衡词典大小与未登录词问题。

#tablem[
  | 阶段 | 核心操作 | 目的与效果 |
  | :--- | :--- | :--- |
  | **1. 数据准备** | 收集文本语料库，准备特殊标记（如 `<｜endoftext｜>`） | 提供训练数据，定义文本边界 |
  | **2. 预分词** | 按特殊标记分割文本，再用正则规则提取基础词元 | 将连续文本拆分为可处理的原子单元 |
  | **3. 频率统计** | 统计所有相邻词元对的共现频率 | 识别文本中最常见的字符组合 |
  | **4. 迭代合并** | 重复合并当前最高频的词对，将其加入词表 | 逐步从字符构建出常见子词和完整单词 |
  | **5. 词表构建** | 记录所有子词及其唯一ID，生成 `vocab.json` 和 `merges.json` | 形成固定的编码-解码映射关系 |
  | **6. 编码应用** | 对新文本应用训练好的合并规则，将其转换为子词ID序列 | 将任意文本转换为模型可处理的数字序列 |
]
---
=== BPE算法详解与作用
**算法简化示例**：
初始语料: `"low lower lowest"`
1. 基础拆分: `l o w` | `l o w e r` | `l o w e s t`
2. 合并高频对 `"lo"`: `lo w` | `lo w e r` | `lo w e s t`
3. 合并高频对 `"low"`: `low` | `low e r` | `low e s t`
4. 最终词表包含: `low`, `e`, `r`, `st`, `er`, `lowest` 等子词

**在项目中的作用**：
- **训练阶段**：分析电影评论文本，自动学习如 `"unhappy"` 拆分为 `["un", "happy"]` 的合并规则
- **推理阶段**：将新评论实时编码为子词ID序列，输入深度学习模型
- **优势**：相比传统分词，能有效处理罕见词和形态学变化